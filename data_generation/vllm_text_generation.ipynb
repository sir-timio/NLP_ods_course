{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf, ListConfig\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from vllm import EngineArgs, LLMEngine, SamplingParams, RequestOutput\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "from spacy.tokens import Span, Doc\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init hydra config\n",
    "\n",
    "initialize(version_base=None, config_path=\"../conf/generation\", job_name=\"test_app\")\n",
    "cfg = compose(config_name=\"generation_conf_quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_cfg = cfg.wandb\n",
    "os.environ[\"WANDB_PROJECT\"] = wandb_cfg.project\n",
    "os.environ[\"WANDB_ENTITY\"] = wandb_cfg.entity\n",
    "os.environ[\"WANDB_JOB_TYPE\"] = wandb_cfg.job_type\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"all\"\n",
    "\n",
    "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the prompt we will be using for rewriting essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read promt_format\n",
    "with open(cfg.prompt_path, \"r\") as file:\n",
    "    prompt_format = file.read()\n",
    "    print(prompt_format.format(\"\\'original essay\\'\", \"\\'PII entities\\'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create requests for the engine with variable **entity types** and **sampling parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import random\n",
    "\n",
    "PII_ENTS = [\n",
    "  (\"name\", \"NAME_STUDENT\", \"James Brown\"), # обрати внимание ФИО это одна сущность или несколько !!!!\n",
    "  (\"email\", \"EMAIL\", \"example@email.com\"),\n",
    "  (\"personal_url\", \"URL_PERSONAL\", \"https://example.com\"),\n",
    "  (\"username\", \"USERNAME\", \"john42\"),\n",
    "  (\"address\", \"STREET_ADDRESS\", \"221B, Baker Street, London\"),\n",
    "  (\"phone_num\", \"PHONE_NUM\", \"+1 212 555 0188\"),\n",
    "  (\"userid\", \"ID_NUM\", \"123456789\")\n",
    "]\n",
    "\n",
    "ENT_COMBINATIONS = [\n",
    "   *[[ent] for ent in PII_ENTS],\n",
    "   *[[PII_ENTS[0], ent] for ent in PII_ENTS[1:]],\n",
    "   *[list(comb) for comb in combinations(PII_ENTS[:4], 3)]\n",
    "]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.engine.model)\n",
    "\n",
    "def sample_ent_combination():\n",
    "   return random.choice(ENT_COMBINATIONS)\n",
    "\n",
    "def dict2str(d):\n",
    "  return \"\\n\".join([f\"{k}={v}\" for k, v in d.items()])\n",
    "\n",
    "def build_request(prompt_format, ent_combination, sampling_params, essay=None):\n",
    "  true_ents_dict = {ent_type: [ent_text] for ent_description, ent_type, ent_text in ent_combination}\n",
    "  pii_str = \"\\n\".join([f\"{ent_description}={ent_text}\" for ent_description, ent_type, ent_text in ent_combination]) \n",
    "  prompt = prompt_format.format(pii_str) if essay is None else prompt_format.format(essay, pii_str)\n",
    "\n",
    "  chat = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    "  prompt_with_chat_template = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "  request = {\n",
    "    \"prompt\": prompt_with_chat_template,\n",
    "    \"true_ents_dict\": true_ents_dict,\n",
    "    \"sampling_params\": sampling_params,\n",
    "    \"lora_params\": None\n",
    "  }\n",
    "  return request\n",
    "\n",
    "def create_requests(essays, cfg) -> List[Tuple[str, SamplingParams, Optional[LoRARequest]]]:\n",
    "    essays = [None] if essays is None else essays\n",
    "    generation_requests = []\n",
    "    for essay in essays:\n",
    "      ent_combination = sample_ent_combination()\n",
    "      request = build_request(prompt_format, \n",
    "                              ent_combination, \n",
    "                              sampling_params=OmegaConf.to_container(cfg.sampling_params), \n",
    "                              essay=essay)\n",
    "      generation_requests.append(request)\n",
    "\n",
    "    return generation_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read essays for rewriting\n",
    "orig_essays_df = pd.read_json(cfg.original_essays_path)\n",
    "orig_essays_df = orig_essays_df[~orig_essays_df[\"has_ents\"]]\n",
    "essays = orig_essays_df[\"full_text\"].tolist()\n",
    "\n",
    "generation_requests = create_requests(essays, cfg)\n",
    "generation_requests = generation_requests[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generation_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed requests to the engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_engine(cfg) -> LLMEngine:\n",
    "    engine_args = EngineArgs(**cfg.engine)\n",
    "    return LLMEngine.from_engine_args(engine_args)\n",
    "\n",
    "def process_requests(engine, generation_requests):\n",
    "    \"\"\"Continuously pro cess a list of prompts and handle the outputs.\"\"\"\n",
    "\n",
    "    generated_examples = []\n",
    "    for request_id, request_data in enumerate(generation_requests):\n",
    "\n",
    "        prompt = request_data[\"prompt\"]\n",
    "        sampling_params = SamplingParams(**request_data[\"sampling_params\"])\n",
    "        lora_request = None if not request_data[\"lora_params\"] else LoRARequest(**request_data[\"lora_params\"])\n",
    "\n",
    "        engine.add_request(str(request_id), prompt, sampling_params, lora_request)\n",
    "\n",
    "\n",
    "    while engine.has_unfinished_requests():\n",
    "        request_outputs = engine.step()\n",
    "        for request_output in request_outputs:\n",
    "            if request_output.finished:\n",
    "                for output in request_output.outputs:\n",
    "                    generated_text = output.text\n",
    "                    request_data = generation_requests[int(request_output.request_id)]\n",
    "                    generated_examples.append({\"generated_text\": generated_text, **request_data})\n",
    "    return generated_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = initialize_engine(cfg)    \n",
    "generated_examples = process_requests(engine, generation_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_df = pd.DataFrame().from_records(generated_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace generated entities with their accrording labels\n",
    "generation_df = generation_df.agg(replace_ents_with_labels, axis=1)\n",
    "\n",
    "# Tokenize texts (with labels instead of entities)\n",
    "generation_df = generation_df.agg(tokenize_df_with_spacy, axis=1)\n",
    "\n",
    "# Find entities and mark their positions\n",
    "generation_df = generation_df.agg(mark_ent_label_tokens, axis=1)\n",
    "\n",
    "# Replace labels with new entities\n",
    "# generation_df = generation_df.agg(replace_labels_with_ents, axis=1)\n",
    "\n",
    "generation_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = generation_df[[\"tokens\", \"trailing_whitespace\", \"labels\"]].applymap(len)\n",
    "mask = (len_df[\"tokens\"] == len_df[\"trailing_whitespace\"]) & (len_df[\"tokens\"] == len_df[\"labels\"])\n",
    "assert mask.astype(int).agg(\"prod\") == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysys of generated data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = generation_df.iloc[0]\n",
    "print(set(row[\"labels\"]))\n",
    "html = visualize_ents(row[\"tokens\"], row[\"trailing_whitespace\"], row[\"labels\"])\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of entities present in generated text\")\n",
    "\n",
    "total_ents = generation_df[\"ents_present_in_generated_text\"].apply(lambda x : [k for k, v in x.items()]).explode().value_counts().to_dict()\n",
    "missing_ents_dict = generation_df[\"ents_present_in_generated_text\"].apply(lambda x : [k for k, v in x.items() if v == True]).explode().value_counts().to_dict()\n",
    "\n",
    "for k in total_ents.keys():\n",
    "    print(f\"{k} -> {missing_ents_dict[k] if k in missing_ents_dict else 0} out of {total_ents[k]} present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most popular enity combinations\")\n",
    "generation_df[\"ents_present_in_generated_text\"].apply(lambda x : [k for k, v in x.items() if v == True]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ent_pos = []\n",
    "for label2pos in generation_df[\"label2position\"]:\n",
    "    for label, positions in label2pos.items():\n",
    "        for pos in positions:\n",
    "            all_ent_pos.append({\"ent\": label, \"pos\": pos})\n",
    "\n",
    "ents_pos_distr_df = pd.DataFrame().from_records(all_ent_pos)\n",
    "\n",
    "# sns.displot(ents_pos_distr_df, x=\"pos\", col=\"ent\")\n",
    "sns.displot(ents_pos_distr_df, x=\"pos\", hue=\"ent\", multiple=\"stack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_visualization(row):\n",
    "    html = visualize_ents(row[\"tokens\"], row[\"trailing_whitespace\"], row[\"labels\"])\n",
    "    row[\"vizualization\"] = wandb.Html(html)\n",
    "    return row\n",
    "\n",
    "log_df = generation_df.apply(add_visualization, axis=1)\n",
    "log_df = log_df.drop(columns=[\"ents_present_in_generated_text\"])\n",
    "\n",
    "ents_present_in_generated_text = pd.json_normalize(generation_df[\"ents_present_in_generated_text\"])\n",
    "log_df = pd.concat([log_df, ents_present_in_generated_text], axis=1)\n",
    "log_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(name=cfg.wandb.run_name, job_type=cfg.wandb.job_type) as run:\n",
    "    table = wandb.Table(dataframe=log_df)\n",
    "    run.summary[\"generated_texts\"] = table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
