{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random, string, re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "UNIQUE_LABELS = {\n",
    "    \"URL_PERSONAL\",\n",
    "    \"EMAIL\",\n",
    "    \"USERNAME\",\n",
    "    \"STREET_ADDRESS\",\n",
    "    \"NAME_STUDENT\",\n",
    "    \"PHONE_NUM\",\n",
    "    \"ID_NUM\"\n",
    "}\n",
    "\n",
    "LABEL2ENT_SPECIAL_TOKEN = {l : l + \"_TOKEN\" for l in UNIQUE_LABELS}\n",
    "ENT_SPECIAL_TOKEN2LABEL = {l + \"_TOKEN\": l for l in UNIQUE_LABELS}\n",
    "\n",
    "ENTITY_SPECIAL_TOKENS = set(LABEL2ENT_SPECIAL_TOKEN.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load generated essays an associated entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_essays_path = \"/home/savkin/2024/PII_Data_Detection/pii/generated_datasets/generated_df_with_essays_final.csv\"\n",
    "\n",
    "df = pd.read_csv(generated_essays_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace generated ents with class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulate ents into dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entities_to_label_dict(row):\n",
    "    label_dict={\"STREET_ADDRESS\": [row[\"STREET_ADDRESS\"]],\n",
    "                \"NAME_STUDENT\": [row[\"NAME_STUDENT\"]],\n",
    "                \"EMAIL\": [row[\"EMAIL\"]],\n",
    "                \"PHONE_NUM\": [row[\"PHONE_NUM\"]],\n",
    "                \"ID_NUM\": [row[\"ID_NUM\"]],\n",
    "                \"USERNAME\": [row[\"USERNAME\"]],\n",
    "                \"URL_PERSONAL\": [row[\"URL_PERSONAL\"]]}\n",
    "    return label_dict\n",
    "\n",
    "df[\"label_dict\"] = df.agg(entities_to_label_dict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_order = [\n",
    "    \"URL_PERSONAL\",\n",
    "    \"EMAIL\",\n",
    "    \"USERNAME\",\n",
    "    \"STREET_ADDRESS\",\n",
    "    \"NAME_STUDENT\",\n",
    "    \"PHONE_NUM\",\n",
    "    \"ID_NUM\",\n",
    "]\n",
    "\n",
    "def replace_ents_with_labels(row):\n",
    "    text = row[\"generated_text\"]\n",
    "    missing_ents_dict = {}\n",
    "    for ent_label in replace_order:\n",
    "        ent = row[\"label_dict\"][ent_label]\n",
    "        assert len(ent) == 1\n",
    "        ent_text = ent[0]\n",
    "        missing_ents_dict[ent_label] = ent_text not in text \n",
    "        if not missing_ents_dict[ent_label]:\n",
    "            \n",
    "            text = text.replace(ent_text, LABEL2ENT_SPECIAL_TOKEN[ent_label])\n",
    "\n",
    "    row[\"generated_text_with_ent_labels\"] = text\n",
    "    row[\"missing_ents_dict\"] = missing_ents_dict\n",
    "    return row\n",
    "\n",
    "df = df.agg(replace_ents_with_labels, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "en_tokenizer = English().tokenizer\n",
    "    \n",
    "def tokenize_with_spacy(text, tokenizer=en_tokenizer):\n",
    "    tokenized_text = tokenizer(text)\n",
    "    tokens = [token.text for token in tokenized_text]\n",
    "    trailing_whitespace = [bool(token.whitespace_) for token in tokenized_text]\n",
    "    return tokens, trailing_whitespace\n",
    "\n",
    "def tokenize_df_with_spacy(row):\n",
    "    tokens, trailing_whitespace = tokenize_with_spacy(row[\"generated_text_with_ent_labels\"])\n",
    "    row[\"tokens\"] = tokens\n",
    "    row[\"trailing_whitespace\"] = trailing_whitespace\n",
    "    row[\"labels\"] = [\"O\"] * len(tokens)\n",
    "    return row\n",
    "\n",
    "df = df.agg(tokenize_df_with_spacy, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find ents positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_ent_label_tokens(row):\n",
    "    ent2pos = {x: [] for x in UNIQUE_LABELS}\n",
    "    tokens = row[\"tokens\"]\n",
    "    for i, tok in enumerate(tokens):\n",
    "        if tok in ENTITY_SPECIAL_TOKENS:\n",
    "            enity_label = ENT_SPECIAL_TOKEN2LABEL[tok]\n",
    "            ent2pos[enity_label].append(i)\n",
    "    row[\"ent2pos\"] = ent2pos\n",
    "    return row\n",
    "\n",
    "df = df.agg(mark_ent_label_tokens, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace labels with ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_labels_with_ents(row):\n",
    "    ent2pos = row[\"ent2pos\"]\n",
    "    ents_dict = row[\"label_dict\"]\n",
    "\n",
    "    entity_mentions = [(ent_label, ent_text, pos) for ent_label, ent_text in ents_dict.items() for pos in ent2pos[ent_label]]\n",
    "    sorted_entity_mentions = sorted(entity_mentions, key=lambda x: x[-1], reverse=True)\n",
    "\n",
    "    for ent_label, ent_text, pos in sorted_entity_mentions:\n",
    "        assert len(ent_text) == 1\n",
    "        ent_text = ent_text[0]\n",
    "        ent_tokens, ent_trailing_whitespace = tokenize_with_spacy(ent_text)\n",
    "        ent_bio_tags = [\"B-\" + ent_label] + [\"I-\" + ent_label] * (len(ent_tokens) - 1)\n",
    "\n",
    "        assert len(ent_tokens) == len(ent_trailing_whitespace) == len(ent_bio_tags)\n",
    "            \n",
    "        for k, v in [(\"tokens\", ent_tokens), (\"trailing_whitespace\", ent_trailing_whitespace), (\"labels\", ent_bio_tags)]:\n",
    "            row[k].pop(pos)\n",
    "            row[k][pos:pos] = v\n",
    "\n",
    "    return row\n",
    "\n",
    "df = df.agg(replace_labels_with_ents, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = df[[\"tokens\", \"trailing_whitespace\", \"labels\"]].applymap(len)\n",
    "mask = (len_df[\"tokens\"] == len_df[\"trailing_whitespace\"]) & (len_df[\"tokens\"] == len_df[\"labels\"])\n",
    "assert mask.astype(int).agg(\"prod\") == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "save_path = \"/home/savkin/2024/PII_Data_Detection/pii/generated_datasets/deleteme.json\"\n",
    "records = df.to_dict(orient=\"records\")\n",
    "\n",
    "with open(save_path, \"w\") as file:\n",
    "    json.dump(records, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated data analysys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
