{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add some statistic features to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\n",
    "\n",
    "df = pd.read_json(\"/archive/savkin/parsed_datasets/NER/PII_Data_Detection/orig_train.json\")\n",
    "\n",
    "df[\"short_labels\"] = df[\"labels\"].apply(lambda labels: [l[2:] if l != \"O\" else \"OTHER\" for l in labels])\n",
    "df[\"has_ents\"] = df['labels'].apply(lambda labels: len(set(labels)) > 1)\n",
    "df[\"tokens_cnt\"] = df[\"labels\"].apply(len)\n",
    "\n",
    "unique_labels = set(id2label.values()) - set([\"O\"])\n",
    "unique_short_labels = df[\"short_labels\"].explode().unique()\n",
    "for label in unique_labels:\n",
    "    df[label] = df[\"labels\"].apply(lambda labels: len([l for l in labels if l == label]))\n",
    "for label in unique_short_labels:\n",
    "    df[label] = df[\"short_labels\"].apply(lambda labels: len([l for l in labels if l == label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in unique_short_labels:\n",
    "    print(f\"Documents count with label {label} -> {len(df[df[label] > 0])}\")\n",
    "# df.explode(\"short_labels\").groupby(\"short_labels\")[\"short_labels\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explode(\"has_ents\").groupby(\"has_ents\")[\"has_ents\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train-valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "df[\"valid\"] = False\n",
    "\n",
    "val_nums = {\n",
    "    'PHONE_NUM': 4,\n",
    "    'STREET_ADDRESS': 2,\n",
    "    'USERNAME': 5,\n",
    "    'EMAIL': 5,\n",
    "    'ID_NUM': 10,\n",
    "    'URL_PERSONAL': 20,\n",
    "    'NAME_STUDENT': 200,\n",
    "    'OTHER': 1500, \n",
    "}\n",
    "\n",
    "# For each class in val_nums, randomly select the specified number of examples and set 'valid' to True\n",
    "for label, num in val_nums.items():\n",
    "    valid_indices = df[df[label] > 0].sample(n=num, replace=False, random_state=42).index\n",
    "    x = len(df[df[\"valid\"] == True])\n",
    "    df.loc[valid_indices, 'valid'] = True\n",
    "\n",
    "\n",
    "# Let's double check the classes per split:\n",
    "for col, _ in val_nums.items():\n",
    "    print(f'VALID {col}: {len(df[(df.valid == True) & (df[col] > 0)])}')\n",
    "    print(f'TRAIN {col}: {len(df[(df.valid == False) & (df[col] > 0)])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render ents with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "options = {\n",
    "    \"colors\": {\n",
    "        \"B-NAME_STUDENT\": \"aqua\",\n",
    "        \"I-NAME_STUDENT\": \"skyblue\",\n",
    "        \"B-EMAIL\": \"limegreen\",\n",
    "        \"I-EMAIL\": \"lime\",\n",
    "        \"B-USERNAME\": \"hotpink\",\n",
    "        \"I-USERNAME\": \"lightpink\",\n",
    "        \"B-ID_NUM\": \"purple\",\n",
    "        \"I-ID_NUM\": \"rebeccapurple\",\n",
    "        \"B-PHONE_NUM\": \"red\",\n",
    "        \"I-PHONE_NUM\": \"salmon\",\n",
    "        \"B-URL_PERSONAL\": \"silver\",\n",
    "        \"I-URL_PERSONAL\": \"lightgray\",\n",
    "        \"B-STREET_ADDRESS\": \"brown\",\n",
    "        \"I-STREET_ADDRESS\": \"chocolate\",\n",
    "    }\n",
    "}\n",
    "\n",
    "def visualize(row):\n",
    "    doc = nlp(row.full_text)\n",
    "    doc.ents = [\n",
    "        Span(doc, idx, idx + 1, label=label)\n",
    "        for idx, label in enumerate(row.labels)\n",
    "        if label != \"O\"\n",
    "    ]\n",
    "    html = displacy.render(doc, style=\"ent\", jupyter=False, options=options)\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "html = visualize(df.loc[0])\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add striding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 512\n",
    "\n",
    "def rebuild_text(tokens, trailing_whitespace):\n",
    "    text = ''\n",
    "    for token, ws in zip(tokens, trailing_whitespace):\n",
    "        ws = \" \" if ws == True else \"\"\n",
    "        text += token + ws\n",
    "    return text\n",
    "\n",
    "stride_df = df.copy(deep=True)\n",
    "new_df = []\n",
    "for _, row in df.iterrows():\n",
    "    for start_tok_id in range(0, len(row[\"tokens\"]), stride):\n",
    "        if len(row[\"tokens\"]) <= stride:\n",
    "            new_df.append(row.to_dict())\n",
    "            continue\n",
    "\n",
    "        end_tok_id = start_tok_id + stride\n",
    "\n",
    "        new_row = {}\n",
    "        new_row['document'] = row['document']\n",
    "        new_row['valid'] = row['valid']\n",
    "        new_row['tokens'] = row[\"tokens\"][start_tok_id:end_tok_id]\n",
    "        new_row['trailing_whitespace'] = row['trailing_whitespace'][start_tok_id:end_tok_id]\n",
    "        new_row['labels'] = row['labels'][start_tok_id:end_tok_id]\n",
    "        \n",
    "        \n",
    "\n",
    "        token_id2pos = lambda id: sum([len(row[\"tokens\"][i]) + int(row['trailing_whitespace'][i]) for i in range(0, id + 1)])\n",
    "        doc_slice_start = token_id2pos(start_tok_id)\n",
    "        doc_slice_end = token_id2pos(min(end_tok_id - 1, len(row[\"tokens\"]) - 1))\n",
    "        new_row['full_text'] = rebuild_text(new_row['tokens'], new_row['trailing_whitespace'])\n",
    "        \n",
    "\n",
    "        new_df.append(new_row)\n",
    "split_df = pd.DataFrame(new_df)\n",
    "split_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = split_df[split_df[\"document\"] == 9980].reset_index()\n",
    "# print(x[3][-10:])\n",
    "# print(x[4][10:])\n",
    "# print(x[x[\"index\"] == 3][\"full_text\"][-100:])\n",
    "# print(x[x[\"index\"] == 4][\"full_text\"][:100])\n",
    "a  = x[x[\"index\"] == 3][\"full_text\"][0]\n",
    "b  = x[x[\"index\"] == 4][\"full_text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(a))\n",
    "print(len(b))\n",
    "print(len(b)+len(a))\n",
    "print(len(df[df[\"document\"] == 9980][\"full_text\"][1158]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x[\"labels\"][3]))\n",
    "print(len(x[\"labels\"][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"document\"] == 9980][\"full_text\"][1158]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"visualization\"] = [wandb.Html(visualize(row)) for _, row in df.iterrows()]\n",
    "split_df[\"visualization\"] = [wandb.Html(visualize(row)) for _, row in split_df.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"PII Data Detection\", job_type=\"load-data\")\n",
    "\n",
    "raw_data = wandb.Artifact(\n",
    "    name=\"raw_dataset\",\n",
    "    type=\"dataset\"\n",
    ")\n",
    "\n",
    "table = wandb.Table(data=df)\n",
    "raw_data.add(table, \"df_table\")\n",
    "wandb.log_artifact(raw_data)\n",
    "\n",
    "# processed_data = wandb.Artifact(\n",
    "#     name=f'stride_{stride}_dataset', \n",
    "#     type=\"dataset\")\n",
    "# processed_data.add_file(f'stride_{stride}_dataset.parquet')\n",
    "# wandb.log_artifact(processed_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
