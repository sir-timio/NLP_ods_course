{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"PII Data Detection\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"true\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"all\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForTokenClassification, \n",
    "    AutoTokenizer, \n",
    "    EarlyStoppingCallback, \n",
    "    set_seed\n",
    ")\n",
    "from seqeval.metrics import (\n",
    "    recall_score, \n",
    "    precision_score, \n",
    "    classification_report, \n",
    "    f1_score\n",
    ")\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "from utils import visualize_ents, apply_threshold\n",
    "\n",
    "random_seed = 42\n",
    "set_seed(random_seed)\n",
    "\n",
    "INFERENCE_MAX_LENGTH = 1024\n",
    "wandb_run_name = f\"deberta-base-{INFERENCE_MAX_LENGTH}-crf\"\n",
    "# wandb_run_name = \"deleteme\"\n",
    "model_save_path = f\"/archive/savkin/models/ner/PII Data Detection/{wandb_run_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and convert ner_tags to labels\n",
    "allowed_cols = ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels', 'valid']\n",
    "\n",
    "df = pd.read_json(\"/archive/savkin/parsed_datasets/NER/PII_Data_Detection/orig_train_custom_split.json\")[allowed_cols]\n",
    "\n",
    "id2label = {0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\n",
    "label2id = {v:k for k,v in id2label.items()}\n",
    "O_label_id = label2id['O']\n",
    "\n",
    "\n",
    "df[\"ner_tags\"] = df[\"labels\"].apply(lambda labels_list: [label2id[x] for x in labels_list])\n",
    "df[\"has_ents\"] = df['labels'].apply(lambda labels: len(set(labels)) > 1)\n",
    "\n",
    "train_df = df[df[\"valid\"] == False].reset_index()\n",
    "valid_df = df[df[\"valid\"] == True].reset_index()\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"valid\": Dataset.from_pandas(valid_df)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 52.0/52.0 [00:00<00:00, 306kB/s]\n",
      "config.json: 100%|██████████| 579/579 [00:00<00:00, 4.21MB/s]\n",
      "spm.model: 100%|██████████| 2.46M/2.46M [00:00<00:00, 19.2MB/s]\n",
      "/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5117/5117 [00:08<00:00, 627.27 examples/s]\n",
      "Map: 100%|██████████| 1690/1690 [00:02<00:00, 563.56 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'word_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand word labels to tokens labels\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "# Tokenize dataset and align labels with tokens\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        max_length=INFERENCE_MAX_LENGTH, \n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    subtoken2word = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        subtoken2word.append(word_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    tokenized_inputs[\"word_ids\"] = subtoken2word\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 6.34k/6.34k [00:00<00:00, 23.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "seqeval_metrics = evaluate.load(\"seqeval\")\n",
    "\n",
    "def f5_score(precision, recall):\n",
    "    return (1 + 5*5) * recall * precision / (5*5*precision + recall + 1e-100)\n",
    "\n",
    "def compute_metrics_from_labels(predictions, labels):\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval_metrics.compute(predictions=true_predictions, references=true_labels)\n",
    "    for label, scores in results.items():\n",
    "        if \"overall\" not in label:\n",
    "            precision = scores[\"precision\"]\n",
    "            recall = scores[\"recall\"]\n",
    "            results[label][\"f5_score\"] = f5_score(precision, recall)\n",
    "    precision = results[\"overall_precision\"]\n",
    "    recall = results[\"overall_recall\"]\n",
    "    results[\"overall_f5_score\"] = f5_score(precision, recall)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return compute_metrics_from_labels(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, pad_to_multiple_of=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 371M/371M [00:02<00:00, 179MB/s] \n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForTokenClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init training arguments\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, pad_to_multiple_of=16)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"training_logs\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    num_train_epochs=5,\n",
    "    # num_train_epochs=1,\n",
    "    # max_steps=400,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    warmup_steps=600,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    logging_steps=200,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"overall_f5_score\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    "    # report_to=\"none\",\n",
    "    run_name=wandb_run_name\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"valid\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # callbacks=[EarlyStoppingCallback(4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from saved if needed\n",
    "\n",
    "model_checkpoint = \"/archive/savkin/models/ner/PII Data Detection/deberta-base-4000\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "trainer = Trainer(\n",
    "    args = TrainingArguments(output_dir=\"tmp_trainer\", report_to=\"none\"),\n",
    "    model=model,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(name=f\"{wandb_run_name}-post-evaluation\", job_type=\"post-evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_dataset[\"valid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log metrics depending on the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
    "# thresholds = [0.5, 0.6, 0.7]\n",
    "\n",
    "thresholded_metrics = {}\n",
    "best_threshold = 0\n",
    "for i, threshold in enumerate(thresholds):\n",
    "    preds = predictions.predictions\n",
    "    true_labels = predictions.label_ids\n",
    "    thresholed_pred_labels = apply_threshold(preds, threshold, O_label_id)\n",
    "\n",
    "    metrics = compute_metrics_from_labels(thresholed_pred_labels, true_labels)\n",
    "    thresholded_metrics[threshold] = metrics\n",
    "    f5 = metrics[\"overall_f5_score\"]\n",
    "    print(f\"Threshold {threshold}, overall_f5_score = {f5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics based on threshold\n",
    "run.define_metric(\"threshold\")\n",
    "run.define_metric(f\"thresholded_*\", step_metric=\"threshold\", summary=\"max\")\n",
    "\n",
    "for threshold, metrics in thresholded_metrics.items():\n",
    "    for metric_name, metric in metrics.items():\n",
    "        new_metric_name = f\"thresholded_{metric_name}\"\n",
    "        run.log({new_metric_name: metric, \"threshold\": threshold})\n",
    "        # print({new_metric_name: metric, \"threshold\": threshold})\n",
    "    \n",
    "thresholed_f5_scores = [metric[\"overall_f5_score\"] for _, metric in thresholded_metrics.items()]\n",
    "best_threshold_id = np.argmax(thresholed_f5_scores)\n",
    "best_threshold = thresholds[best_threshold_id]\n",
    "run.summary[\"best_overall_threshold\"] = best_threshold\n",
    "run.summary[\"best_overall_f5_score\"] = thresholed_f5_scores[best_threshold_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate subtoken-level predictions into word-level predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probas = softmax(predictions.predictions, axis=-1).max(-1)\n",
    "pred_labels = apply_threshold(predictions.predictions, best_threshold, O_label_id)\n",
    "true_labels = predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = {\n",
    "    \"row_id\": [],\n",
    "    \"document\": [],\n",
    "    \"token\": [],\n",
    "    \"label\": [],\n",
    "    \"subtoken_str\": [],\n",
    "    \"word_str\": [],\n",
    "    \"proba\": []\n",
    "}\n",
    "\n",
    "for input_ids, word_ids, row_id, document, words, p_labels, p_probas in zip(tokenized_dataset[\"valid\"][\"input_ids\"], \n",
    "                                                                            tokenized_dataset[\"valid\"][\"word_ids\"], \n",
    "                                                                            valid_df.index, \n",
    "                                                                            valid_df[\"document\"], \n",
    "                                                                            valid_df[\"tokens\"], \n",
    "                                                                            pred_labels, \n",
    "                                                                            pred_probas):\n",
    "    subtokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    for subtoken_id, (subtoken, label_id, proba) in enumerate(zip(subtokens, p_labels, p_probas)):\n",
    "        word_id = word_ids[subtoken_id]\n",
    "        if label_id != -100 and label_id != O_label_id and word_id is not None: # ignore O-labels\n",
    "            submission[\"row_id\"].append(row_id)\n",
    "            submission[\"document\"].append(document)\n",
    "            submission[\"token\"].append(word_id)\n",
    "            submission[\"label\"].append(id2label[label_id])\n",
    "            submission[\"subtoken_str\"].append(subtoken)\n",
    "            submission[\"word_str\"].append(words[word_id])\n",
    "            submission[\"proba\"].append(proba)\n",
    "\n",
    "\n",
    "df = pd.DataFrame().from_dict(submission).drop_duplicates().sort_values(by=[\"document\", \"token\"])\n",
    "# submission_df_subtoken_level = df[df[\"label\"] != \"O\"].copy(deep=True)[[\"row_id\", \"document\", \"token\", \"label\", \"proba\"]]\n",
    "            \n",
    "subtoken_df =  df[df[\"label\"] != \"O\"].copy(deep=True)\n",
    "subtoken_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_subtokens(df, label_agg_type = \"most_frequent\", add_subtoken_info = False):\n",
    "    df = df.reset_index()\n",
    "    row = df.iloc[0]\n",
    "\n",
    "    if add_subtoken_info:\n",
    "        row[\"subtokens\"] = df[\"subtoken_str\"].agg(lambda x: x.tolist())\n",
    "        row[\"probas\"] = df[\"proba\"].agg(lambda x: x.tolist())\n",
    "\n",
    "    if label_agg_type == \"most_frequent\":\n",
    "        row[\"label\"] = df.groupby([\"label\"])[\"row_id\"].count().sort_values().index[-1]\n",
    "        row[\"agg_proba\"] = df[df[\"label\"] == row[\"label\"]][\"proba\"].agg(\"mean\")\n",
    "    elif label_agg_type == \"first\":\n",
    "        row[\"label\"] = df[\"label\"].agg(lambda x: x[0])\n",
    "        row[\"agg_proba\"] = df[\"proba\"].agg(lambda x: x[0])\n",
    "    elif label_agg_type == \"max_proba\":\n",
    "        row[\"label\"] = df.iloc[df[\"proba\"].idxmax()][\"label\"]\n",
    "        row[\"agg_proba\"] = df[\"proba\"].agg(\"max\")\n",
    "    \n",
    "    return row\n",
    "\n",
    "# submission_df = submission_df_subtoken_level.groupby([\"document\", \"token\"]) \\\n",
    "#                                             .apply(aggregate_subtokens, label_agg_type=\"most_frequent\") \\\n",
    "#                                             .reset_index(drop=True) \\\n",
    "#                                             .drop(columns=[\"index\", \"proba\", \"agg_proba\"])\n",
    "\n",
    "word_df = subtoken_df.groupby([\"document\", \"token\"]) \\\n",
    "                     .apply(aggregate_subtokens, add_subtoken_info=True) \\\n",
    "                     .reset_index(drop=True) \\\n",
    "                     .drop(columns=[\"index\", \"subtoken_str\", \"proba\"])\n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging word-level predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log word-level metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_word_labels = valid_df[\"labels\"].apply(lambda labels: [label2id[l] for l in labels]).tolist()\n",
    "\n",
    "# Create a template filled with \"O\" label\n",
    "pred_word_labels = valid_df[\"labels\"].agg(lambda x: [O_label_id for _ in x]).tolist()\n",
    "\n",
    "# Group words into documents and reorder documents according to validation dataset\n",
    "original_document_order = valid_df[\"document\"].tolist()\n",
    "document_df = word_df[[\"document\", \"token\", \"label\"]].groupby(\"document\").agg(list)\n",
    "reordered_document_df = document_df.reindex(original_document_order, fill_value=[])\n",
    " \n",
    "# Add predictions to the template\n",
    "for i, (_, row) in enumerate(reordered_document_df.iterrows()):\n",
    "    if len(row[\"token\"]) > 0:\n",
    "        for token_id, l in zip(row[\"token\"], row[\"label\"]):\n",
    "            pred_word_labels[i][token_id] = label2id[l]\n",
    "    \n",
    "word_level_metrics = compute_metrics_from_labels(pred_word_labels, true_word_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.define_metric(f\"word_level*\")\n",
    "for metric_name, metric in word_level_metrics.items():\n",
    "    new_metric_name = f\"word_level_{metric_name}\"\n",
    "    run.summary[new_metric_name] = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log model mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rows = []\n",
    "for (_, valid_row), pred_doc_labels, true_doc_labels in zip(valid_df.iterrows(), pred_word_labels, true_word_labels): \n",
    "\n",
    "    pred_doc_labels = np.array(pred_doc_labels)\n",
    "    true_doc_labels = np.array(true_doc_labels)\n",
    "    errors_mask = pred_doc_labels != true_doc_labels\n",
    "\n",
    "    if sum(errors_mask) == 0:\n",
    "        continue\n",
    "\n",
    "    words = (valid_row[\"tokens\"])\n",
    "    trailing_whitespaces = valid_row[\"trailing_whitespace\"]\n",
    "    doc_id = valid_row[\"document\"]\n",
    "\n",
    "\n",
    "    error_pred_labels = pred_doc_labels[errors_mask]\n",
    "    error_true_labels = true_doc_labels[errors_mask]\n",
    "    error_words = np.array(words)[errors_mask]\n",
    "    error_word_ids = np.argwhere(errors_mask)\n",
    "\n",
    "\n",
    "    target_vizualization = wandb.Html(visualize_ents(words, trailing_whitespaces, [id2label[l] for l in true_doc_labels]))\n",
    "    pred_vizualization = wandb.Html(visualize_ents(words, trailing_whitespaces, [id2label[l] for l in pred_doc_labels]))\n",
    "\n",
    "    row = {}\n",
    "    for w, w_id, p_l, t_l in zip(error_words, error_word_ids, error_pred_labels, error_true_labels):\n",
    "        w_id = w_id[0]\n",
    "        pred_row = word_df[word_df[\"document\"] == doc_id]\n",
    "        pred_row = pred_row[pred_row[\"token\"] == w_id]\n",
    "\n",
    "        row[\"document\"] = doc_id\n",
    "        row[\"word\"] = w\n",
    "        row[\"word_id\"] = w_id\n",
    "        row[\"pred_label\"] = id2label[p_l]\n",
    "        row[\"true_label\"] = id2label[t_l]\n",
    "        row[\"target_viz\"] = target_vizualization\n",
    "        row[\"pred_viz\"] = pred_vizualization\n",
    "\n",
    "        assert len(pred_row) <= 1\n",
    "\n",
    "        if len(pred_row) == 1:       \n",
    "            row[\"subtokens\"] = pred_row[\"subtokens\"].to_numpy().squeeze()\n",
    "            row[\"probas\"] = pred_row[\"probas\"].tolist()[0]\n",
    "            row[\"agg_proba\"] = pred_row[\"agg_proba\"].tolist()[0]\n",
    "        elif len(pred_row) == 0:\n",
    "            row[\"subtokens\"] = None\n",
    "            row[\"probas\"] = None\n",
    "            row[\"agg_proba\"] = None\n",
    "    error_rows.append(row)\n",
    "    \n",
    "\n",
    "error_df = pd.DataFrame().from_records(error_rows).sort_values(by=[\"document\", \"word_id\"])\n",
    "error_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_table = wandb.Table(dataframe=error_df)\n",
    "run.summary[\"error_table\"] = error_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
