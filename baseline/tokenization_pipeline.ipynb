{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1022127/1712808105.py:31: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForTokenClassification, \n",
    "    AutoTokenizer, \n",
    "    EarlyStoppingCallback, \n",
    "    set_seed\n",
    ")\n",
    "from seqeval.metrics import (\n",
    "    recall_score, \n",
    "    precision_score, \n",
    "    classification_report, \n",
    "    f1_score\n",
    ")\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import logging\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Span, Doc\n",
    "from spacy import displacy\n",
    "from spacy.lang.en import English\n",
    "import numpy as np\n",
    "\n",
    "from utils import *\n",
    "from colorama import Fore\n",
    "\n",
    "SEQ_MAX_LENGTH = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\n",
    "label2id = {v:k for k,v in id2label.items()}\n",
    "O_label_id = label2id['O']\n",
    "\n",
    "\n",
    "allowed_cols = ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels', 'valid']\n",
    "df = pd.read_json(\"/archive/savkin/parsed_datasets/NER/PII_Data_Detection/orig_train_custom_split.json\")[allowed_cols]\n",
    "df[\"ner_tags\"] = df[\"labels\"].apply(lambda labels_list: [label2id[x] for x in labels_list])\n",
    "\n",
    "train_df = df[df[\"valid\"] == False].reset_index()\n",
    "valid_df = df[df[\"valid\"] == True].reset_index()\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"valid\": Dataset.from_pandas(valid_df)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that word tokenization was done with __spacy English().tokenizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5117it [00:11, 461.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for _, row in tqdm(train_df.iterrows()):\n",
    "    text = row.full_text\n",
    "    spacy_tokens, spacy_whitespaces = tokenize_with_spacy(text)\n",
    "    assert(row.tokens == spacy_tokens)\n",
    "    assert(row.trailing_whitespace == spacy_whitespaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that full text can be recovered from words and trailing whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5117it [00:00, 6527.50it/s]\n"
     ]
    }
   ],
   "source": [
    "for _, row in tqdm(train_df.iterrows()):\n",
    "    new_text = \"\".join([token + (\" \" if whitespace else \"\") for token, whitespace in zip(row.tokens, row.trailing_whitespace)])\n",
    "    assert(row.full_text == new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original NER tags are marked according to the BIO format:\n",
    "\n",
    "_B-_ tag corresponds to the start word of entity\n",
    "\n",
    "_I-_ tag corresponds to the innner words of entity\n",
    "\n",
    "_O_ tag means that word doesn't belong to any entities\n",
    "\n",
    "Let's have a look at example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hellow worldd Russia!\n",
      "Words: ['Hellow', 'worldd', 'Russia', '!']\n",
      "NER tags: ['O', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'O']\n",
      "Trailing_whitespace: [True, True, False, False]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hellow \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    worldd Russia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET_ADDRESS</span>\n",
       "</mark>\n",
       "!</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_text = \"Hellow worldd Russia!\"\n",
    "example_text_tokens, example_text_trailing_whitespace = tokenize_with_spacy(example_text)\n",
    "example_text_ents=[\"O\", \"B-STREET_ADDRESS\", \"I-STREET_ADDRESS\", \"O\"]\n",
    "example_text_labels = [12, 4, 10, 12]\n",
    "\n",
    "print(f\"Text: {example_text}\")\n",
    "print(f\"Words: {example_text_tokens}\")\n",
    "print(f\"NER tags: {example_text_ents}\")\n",
    "print(f\"Trailing_whitespace: {example_text_trailing_whitespace}\")\n",
    "\n",
    "html = visualize_ents(example_text_tokens, example_text_trailing_whitespace, example_text_ents)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtoken-level tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we further split words into subtokens and align them with original NER tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  ['Hellow', 'worldd', 'Russia', '!']\n",
      "Subtokens:  ['[CLS]', '▁Hello', 'w', '▁world', 'd', '▁Russia', '▁!', '[SEP]']\n",
      "Mapping between words ans subtokens:  [None, 0, 0, 1, 1, 2, 3, None]\n",
      "\n",
      "New NER tags: \n",
      "[CLS]\t-> SPECIAL TOKEN \n",
      "▁Hello\t-> O \n",
      "w\t-> O \n",
      "▁world\t-> B-STREET_ADDRESS \n",
      "d\t-> B-STREET_ADDRESS \n",
      "▁Russia\t-> I-STREET_ADDRESS \n",
      "▁!\t-> O \n",
      "[SEP]\t-> SPECIAL TOKEN \n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "checkpoint_path = f\"/archive/savkin/models/ner/PII Data Detection/deberta-base-4000\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Expand word labels to tokens labels\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "inputs = tokenizer(example_text_tokens, is_split_into_words=True)\n",
    "labels = example_text_labels\n",
    "word_ids = inputs.word_ids()\n",
    "subtokens = inputs.tokens()\n",
    "token_labels = align_labels_with_tokens(labels, word_ids)\n",
    "\n",
    "\n",
    "print(\"Words: \", example_text_tokens)\n",
    "print(\"Subtokens: \", subtokens)\n",
    "print(\"Mapping between words ans subtokens: \", word_ids)\n",
    "print(\"\\nNew NER tags: \")\n",
    "for token, token_label in list(zip(subtokens, token_labels))[:20]:\n",
    "    token_ner_tag = \"SPECIAL TOKEN\" if token_label == -100 else id2label[token_label]\n",
    "    print(f\"{token}\\t-> {token_ner_tag} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can tokenize the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86da01b3bea44e3fb69f7016cb13e041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2916059c9582451c8e5fdb97e3b91e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1690 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'word_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize dataset and align labels with tokens\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        max_length=SEQ_MAX_LENGTH, \n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    subtoken2word = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        subtoken2word.append(word_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    tokenized_inputs[\"word_ids\"] = subtoken2word\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenize_func = tokenize_and_align_labels\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_func,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "tokenized_dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets's start from looking at AutoModelForTokenClassification predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6c6010556c43c99a3402f12854737b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize example text\n",
    "example_df = pd.DataFrame.from_records([\n",
    "    {\n",
    "        \"full_text\": example_text,\n",
    "        \"tokens\": example_text_tokens,\n",
    "        \"labels\": example_text_ents,\n",
    "        \"trailing_whitespace\": example_text_trailing_whitespace,\n",
    "        \"ner_tags\": example_text_labels\n",
    "    }]\n",
    ")\n",
    "example_dataset = Dataset.from_pandas(example_df)\n",
    "tokenized_example = example_dataset.map(\n",
    "    tokenize_func,\n",
    "    batched=True,\n",
    "    remove_columns=example_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "model = AutoModelForTokenClassification.from_pretrained(checkpoint_path)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    args = TrainingArguments(output_dir=\"tmp_trainer\"),\n",
    "    model=model,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mИстинная разметка: \u001b[37m\n",
      "[CLS]\t-> SPECIAL TOKEN \n",
      "▁Hello\t-> O \n",
      "w\t-> O \n",
      "▁world\t-> B-STREET_ADDRESS \n",
      "d\t-> B-STREET_ADDRESS \n",
      "▁Russia\t-> I-STREET_ADDRESS \n",
      "▁!\t-> O \n",
      "[SEP]\t-> SPECIAL TOKEN \n",
      "\n",
      "\u001b[33mПредсказания модели \u001b[37m\n",
      "[CLS]\t-> SPECIAL TOKEN \n",
      "▁Hello\t-> O \n",
      "w\t-> O \n",
      "▁world\t-> O \n",
      "d\t-> O \n",
      "▁Russia\t-> O \n",
      "▁!\t-> O \n",
      "[SEP]\t-> SPECIAL TOKEN \n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "predictions = trainer.predict(tokenized_example)\n",
    "\n",
    "true_labels = predictions.label_ids[0]\n",
    "pred_labels = predictions.predictions.argmax(-1)[0]\n",
    "\n",
    "print(Fore.GREEN + \"Истинная разметка:\", Fore.WHITE)\n",
    "for token, true_l in list(zip(subtokens, true_labels))[:20]:\n",
    "    token_ner_tag = \"SPECIAL TOKEN\" if true_l == -100 else id2label[true_l]\n",
    "    print(f\"{token}\\t-> {token_ner_tag} \")\n",
    "\n",
    "print()\n",
    "print(Fore.YELLOW + \"Предсказания модели\", Fore.WHITE)\n",
    "for token, pred_l, true_l in list(zip(subtokens, pred_labels, true_labels))[:20]:\n",
    "    token_ner_tag = \"SPECIAL TOKEN\" if true_l == -100 else id2label[pred_l]\n",
    "    print(f\"{token}\\t-> {token_ner_tag} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation of subtoken-level predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulate subtoken-level predictions into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Infer model\n",
    "predictions = trainer.predict(tokenized_dataset[\"valid\"])\n",
    "true_labels = predictions.label_ids\n",
    "pred_probas = softmax(predictions.predictions, axis=-1).max(-1)\n",
    "pred_labels = predictions.predictions.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>subtoken_str</th>\n",
       "      <th>word_str</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2528</th>\n",
       "      <td>917</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>▁Nathalie</td>\n",
       "      <td>Nathalie</td>\n",
       "      <td>0.990726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2529</th>\n",
       "      <td>917</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>▁S</td>\n",
       "      <td>Sylla</td>\n",
       "      <td>0.964352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2530</th>\n",
       "      <td>917</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>ylla</td>\n",
       "      <td>Sylla</td>\n",
       "      <td>0.987722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2531</th>\n",
       "      <td>917</td>\n",
       "      <td>7</td>\n",
       "      <td>482</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>▁Nathalie</td>\n",
       "      <td>Nathalie</td>\n",
       "      <td>0.983120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>917</td>\n",
       "      <td>7</td>\n",
       "      <td>483</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>▁S</td>\n",
       "      <td>Sylla</td>\n",
       "      <td>0.919441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_id  document  token           label subtoken_str  word_str     proba\n",
       "2528     917         7      9  B-NAME_STUDENT    ▁Nathalie  Nathalie  0.990726\n",
       "2529     917         7     10  I-NAME_STUDENT           ▁S     Sylla  0.964352\n",
       "2530     917         7     10  I-NAME_STUDENT         ylla     Sylla  0.987722\n",
       "2531     917         7    482  B-NAME_STUDENT    ▁Nathalie  Nathalie  0.983120\n",
       "2532     917         7    483  I-NAME_STUDENT           ▁S     Sylla  0.919441"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = {\n",
    "    \"row_id\": [],\n",
    "    \"document\": [],\n",
    "    \"token\": [],\n",
    "    \"label\": [],\n",
    "    \"subtoken_str\": [],\n",
    "    \"word_str\": [],\n",
    "    \"proba\": []\n",
    "}\n",
    "\n",
    "for input_ids, word_ids, row_id, document, words, p_labels, p_probas in zip(tokenized_dataset[\"valid\"][\"input_ids\"], \n",
    "                                                                            tokenized_dataset[\"valid\"][\"word_ids\"], \n",
    "                                                                            valid_df.index, \n",
    "                                                                            valid_df[\"document\"], \n",
    "                                                                            valid_df[\"tokens\"], \n",
    "                                                                            pred_labels, \n",
    "                                                                            pred_probas):\n",
    "    subtokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    for subtoken_id, (subtoken, label_id, proba) in enumerate(zip(subtokens, p_labels, p_probas)):\n",
    "        word_id = word_ids[subtoken_id]\n",
    "        if label_id != -100 and label_id != O_label_id and word_id is not None: # ignore O-labels\n",
    "            submission[\"row_id\"].append(row_id)\n",
    "            submission[\"document\"].append(document)\n",
    "            submission[\"token\"].append(word_id)\n",
    "            submission[\"label\"].append(id2label[label_id])\n",
    "            submission[\"subtoken_str\"].append(subtoken)\n",
    "            submission[\"word_str\"].append(words[word_id])\n",
    "            submission[\"proba\"].append(proba)\n",
    "\n",
    "subtoken_df = pd.DataFrame().from_dict(submission).drop_duplicates().sort_values(by=[\"document\", \"token\"])\n",
    "subtoken_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have subtoken-level predictions and word mapping, we can merge subtoken NER tags into word-level tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "      <th>word_str</th>\n",
       "      <th>subtokens</th>\n",
       "      <th>probas</th>\n",
       "      <th>agg_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Nathalie</td>\n",
       "      <td>[▁Nathalie]</td>\n",
       "      <td>[0.9907263517379761]</td>\n",
       "      <td>0.990726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>917</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Sylla</td>\n",
       "      <td>[▁S, ylla]</td>\n",
       "      <td>[0.9643515944480896, 0.9877223968505859]</td>\n",
       "      <td>0.976037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>917</td>\n",
       "      <td>7</td>\n",
       "      <td>482</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Nathalie</td>\n",
       "      <td>[▁Nathalie]</td>\n",
       "      <td>[0.9831204414367676]</td>\n",
       "      <td>0.983120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>917</td>\n",
       "      <td>7</td>\n",
       "      <td>483</td>\n",
       "      <td>I-NAME_STUDENT</td>\n",
       "      <td>Sylla</td>\n",
       "      <td>[▁S, ylla]</td>\n",
       "      <td>[0.9194409847259521, 0.9561810493469238]</td>\n",
       "      <td>0.937811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>7</td>\n",
       "      <td>741</td>\n",
       "      <td>B-NAME_STUDENT</td>\n",
       "      <td>Nathalie</td>\n",
       "      <td>[▁Nathalie]</td>\n",
       "      <td>[0.9903643727302551]</td>\n",
       "      <td>0.990364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0  row_id  document  token           label  word_str    subtokens  \\\n",
       "0     917         7      9  B-NAME_STUDENT  Nathalie  [▁Nathalie]   \n",
       "1     917         7     10  I-NAME_STUDENT     Sylla   [▁S, ylla]   \n",
       "2     917         7    482  B-NAME_STUDENT  Nathalie  [▁Nathalie]   \n",
       "3     917         7    483  I-NAME_STUDENT     Sylla   [▁S, ylla]   \n",
       "4     917         7    741  B-NAME_STUDENT  Nathalie  [▁Nathalie]   \n",
       "\n",
       "0                                    probas  agg_proba  \n",
       "0                      [0.9907263517379761]   0.990726  \n",
       "1  [0.9643515944480896, 0.9877223968505859]   0.976037  \n",
       "2                      [0.9831204414367676]   0.983120  \n",
       "3  [0.9194409847259521, 0.9561810493469238]   0.937811  \n",
       "4                      [0.9903643727302551]   0.990364  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aggregate_subtokens(df, label_agg_type = \"most_frequent\", add_subtoken_info = False):\n",
    "    df = df.reset_index()\n",
    "    row = df.iloc[0]\n",
    "\n",
    "    if add_subtoken_info:\n",
    "        row[\"subtokens\"] = df[\"subtoken_str\"].agg(lambda x: x.tolist())\n",
    "        row[\"probas\"] = df[\"proba\"].agg(lambda x: x.tolist())\n",
    "\n",
    "    if label_agg_type == \"most_frequent\":\n",
    "        row[\"label\"] = df.groupby([\"label\"])[\"row_id\"].count().sort_values().index[-1]\n",
    "        row[\"agg_proba\"] = df[df[\"label\"] == row[\"label\"]][\"proba\"].agg(\"mean\")\n",
    "    elif label_agg_type == \"first\":\n",
    "        row[\"label\"] = df[\"label\"].agg(lambda x: x[0])\n",
    "        row[\"agg_proba\"] = df[\"proba\"].agg(lambda x: x[0])\n",
    "    elif label_agg_type == \"max_proba\":\n",
    "        row[\"label\"] = df.iloc[df[\"proba\"].idxmax()][\"label\"]\n",
    "        row[\"agg_proba\"] = df[\"proba\"].agg(\"max\")\n",
    "    \n",
    "    return row\n",
    "\n",
    "word_df = subtoken_df.groupby([\"document\", \"token\"]) \\\n",
    "                     .apply(aggregate_subtokens, add_subtoken_info=True) \\\n",
    "                     .reset_index(drop=True) \\\n",
    "                     .drop(columns=[\"index\", \"subtoken_str\", \"proba\"])\n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before computing the metrics, we need to add \"O\" labels to our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = valid_df[\"labels\"].apply(lambda labels: [label2id[l] for l in labels]).tolist()\n",
    "\n",
    "# Create a template filled with \"O\" label\n",
    "pred_labels = valid_df[\"labels\"].agg(lambda x: [O_label_id for _ in x]).tolist()\n",
    "\n",
    "# Group words into documents and reorder documents according to validation dataset\n",
    "original_document_order = valid_df[\"document\"].tolist()\n",
    "predicted_ents = word_df[[\"document\", \"token\", \"label\"]].groupby(\"document\").agg(list).reindex(original_document_order, fill_value=[])\n",
    " \n",
    "# Add predictions to the template\n",
    "for i, (_, row) in enumerate(predicted_ents.iterrows()):\n",
    "    if len(row[\"token\"]) > 0:\n",
    "        for token_id, l in zip(row[\"token\"], row[\"label\"]):\n",
    "            pred_labels[i][token_id] = label2id[l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def f5_score(precision, recall):\n",
    "    return (1 + 5*5) * recall * precision / (5*5*precision + recall + 1e-100)\n",
    "\n",
    "def compute_metrics_from_labels(predictions, labels):\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    for label, scores in results.items():\n",
    "        if \"overall\" not in label:\n",
    "            precision = scores[\"precision\"]\n",
    "            recall = scores[\"recall\"]\n",
    "            results[label][\"f5_score\"] = f5_score(precision, recall)\n",
    "    precision = results[\"overall_precision\"]\n",
    "    recall = results[\"overall_recall\"]\n",
    "    results[\"overall_f5_score\"] = f5_score(precision, recall)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/savkin/2024/PII_Data_Detection/env/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EMAIL': {'precision': 0.6538461538461539,\n",
       "  'recall': 0.8947368421052632,\n",
       "  'f1': 0.7555555555555555,\n",
       "  'number': 19,\n",
       "  'f5_score': 0.8822355289421158},\n",
       " 'ID_NUM': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'number': 0,\n",
       "  'f5_score': 0.0},\n",
       " 'NAME_STUDENT': {'precision': 0.9324324324324325,\n",
       "  'recall': 0.7095115681233933,\n",
       "  'f1': 0.8058394160583943,\n",
       "  'number': 778,\n",
       "  'f5_score': 0.7160961979842332},\n",
       " 'PHONE_NUM': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'number': 0,\n",
       "  'f5_score': 0.0},\n",
       " 'STREET_ADDRESS': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'number': 0,\n",
       "  'f5_score': 0.0},\n",
       " 'URL_PERSONAL': {'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'number': 0,\n",
       "  'f5_score': 0.0},\n",
       " 'USERNAME': {'precision': 0.3333333333333333,\n",
       "  'recall': 0.009302325581395349,\n",
       "  'f1': 0.018099547511312215,\n",
       "  'number': 215,\n",
       "  'f5_score': 0.009663631295298274},\n",
       " 'overall_precision': 0.7758152173913043,\n",
       " 'overall_recall': 0.5642292490118577,\n",
       " 'overall_f1': 0.6533180778032036,\n",
       " 'overall_accuracy': 0.9995762978335377,\n",
       " 'overall_f5_score': 0.5702104777999691}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics_from_labels(true_labels, pred_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
