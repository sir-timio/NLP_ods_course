{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1'\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_PROJECT\"] = \"PII Data Detection\"\n",
    "os.environ[\"WANDB_ENTITY\"] = \"deeppavlov_team\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"false\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "from spacy.lang.en import English\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers.data.data_collator import DataCollatorForTokenClassification\n",
    "from transformers.models.deberta_v2 import (\n",
    "    DebertaV2ForTokenClassification,\n",
    "    DebertaV2TokenizerFast,\n",
    ")\n",
    "from transformers.tokenization_utils import PreTrainedTokenizerBase\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "import wandb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42  \n",
    "def seed_everywhere(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everywhere(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1024\n",
    "CONF_THRESH = 0.9\n",
    "LR = 2.5e-5\n",
    "LR_SCHEDULER_TYPE = \"linear\"\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 1\n",
    "EVAL_BATCH_SIZE = 8\n",
    "GRAD_ACCUMULATION_STEPS = 16 // BATCH_SIZE\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "AMP = True\n",
    "FREEZE_EMBEDDING = False\n",
    "FREEZE_LAYERS = 6\n",
    "N_SPLITS = 4\n",
    "NEGATIVE_RATIO = 0.3  # down sample ratio of negative samples in the training set\n",
    "OUTPUT_DIR = \"output\"\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = pd.read_json('/archive/savkin/parsed_datasets/NER/PII_Data_Detection/mpware_mixtral8x7b_v1.1-no-i-username.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "og = pd.read_json('/archive/savkin/parsed_datasets/NER/PII_Data_Detection/train.json')\n",
    "train, val = train_test_split(og, test_size=0.1, stratify=og.labels.apply(lambda x: set(x) == set(['O'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([ext, train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['document'] = train['document'].astype('str')\n",
    "val['document'] = val['document'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS','B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'I-USERNAME', 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {v:k for k,v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n",
    "# MODEL_NAME = \"google/gemma-2b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer, label2id, max_length):\n",
    "    text = []\n",
    "\n",
    "    # these are at the character level\n",
    "    labels = []\n",
    "\n",
    "    for t, l, ws in zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]):\n",
    "\n",
    "        text.append(t)\n",
    "        labels.extend([l]*len(t))\n",
    "\n",
    "        # if there is trailing whitespace\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "\n",
    "\n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=False, max_length=max_length)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    text = \"\".join(text)\n",
    "    token_labels = []\n",
    "\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "\n",
    "        # CLS token\n",
    "        if start_idx + end_idx == 0: \n",
    "            token_labels.append(label2id[\"O\"])\n",
    "            continue\n",
    "\n",
    "        # case when token starts with whitespace\n",
    "        if text[start_idx].isspace():\n",
    "            start_idx += 1\n",
    "        \n",
    "        while start_idx >= len(labels):\n",
    "            start_idx -= 1\n",
    "\n",
    "        token_labels.append(label2id[labels[start_idx]])\n",
    "\n",
    "    length = len(tokenized.input_ids)\n",
    "\n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"labels\": token_labels,\n",
    "        \"length\": length\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_dict({\n",
    "    \"full_text\": train.full_text.values,\n",
    "    \"document\": train.document.values,\n",
    "    \"tokens\": train.tokens.values,\n",
    "    \"trailing_whitespace\": train.trailing_whitespace.values,\n",
    "    \"provided_labels\": train.labels.values, # renamed\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = Dataset.from_dict({\n",
    "    \"full_text\": val.full_text.values,\n",
    "    \"document\": val.document.values,\n",
    "    \"tokens\": val.tokens.values,\n",
    "    \"trailing_whitespace\": val.trailing_whitespace.values,\n",
    "    \"provided_labels\": val.labels.values, # renamed\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=3): 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8818/8818 [00:23<00:00, 369.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = train_ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": MAX_LENGTH}, num_proc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=3): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 681/681 [00:02<00:00, 333.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val_ds = val_ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": MAX_LENGTH}, num_proc=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Danielle.Johnson@sanchez-taylor.com', 'B-EMAIL')\n",
      "('156018640', 'B-USERNAME')\n",
      "('DLLT40781618495931', 'B-ID_NUM')\n",
      "('3268542351', 'B-PHONE_NUM')\n",
      "('http://www.instagram.com/zlawrence', 'B-URL_PERSONAL')\n",
      "('600', 'B-STREET_ADDRESS')\n",
      "('Jeffery', 'I-STREET_ADDRESS')\n",
      "('Parkways', 'I-STREET_ADDRESS')\n",
      "('New', 'I-STREET_ADDRESS')\n",
      "('Jamesside', 'I-STREET_ADDRESS')\n",
      "(',', 'I-STREET_ADDRESS')\n",
      "('MT', 'I-STREET_ADDRESS')\n",
      "('29394', 'I-STREET_ADDRESS')\n",
      "('Danielle', 'B-NAME_STUDENT')\n",
      "('Johnson', 'I-NAME_STUDENT')\n",
      "('Danielle', 'B-NAME_STUDENT')\n",
      "('Johnson', 'I-NAME_STUDENT')\n",
      "('Danielle', 'B-NAME_STUDENT')\n",
      "('Johnson', 'I-NAME_STUDENT')\n",
      "('Danielle', 'B-NAME_STUDENT')\n",
      "('Johnson', 'I-NAME_STUDENT')\n",
      "('Danielle', 'B-NAME_STUDENT')\n",
      "('Johnson', 'I-NAME_STUDENT')\n",
      "****************************************************************************************************\n",
      "('▁Daniel', 'B-EMAIL')\n",
      "('le', 'B-EMAIL')\n",
      "('.', 'B-EMAIL')\n",
      "('John', 'B-EMAIL')\n",
      "('son', 'B-EMAIL')\n",
      "('@', 'B-EMAIL')\n",
      "('s', 'B-EMAIL')\n",
      "('anche', 'B-EMAIL')\n",
      "('z', 'B-EMAIL')\n",
      "('-', 'B-EMAIL')\n",
      "('t', 'B-EMAIL')\n",
      "('ay', 'B-EMAIL')\n",
      "('lor', 'B-EMAIL')\n",
      "('.', 'B-EMAIL')\n",
      "('com', 'B-EMAIL')\n",
      "('▁', 'B-USERNAME')\n",
      "('1', 'B-USERNAME')\n",
      "('5', 'B-USERNAME')\n",
      "('6', 'B-USERNAME')\n",
      "('0', 'B-USERNAME')\n",
      "('1', 'B-USERNAME')\n",
      "('8', 'B-USERNAME')\n",
      "('6', 'B-USERNAME')\n",
      "('4', 'B-USERNAME')\n",
      "('0', 'B-USERNAME')\n",
      "('▁D', 'B-ID_NUM')\n",
      "('LL', 'B-ID_NUM')\n",
      "('T', 'B-ID_NUM')\n",
      "('4', 'B-ID_NUM')\n",
      "('0', 'B-ID_NUM')\n",
      "('7', 'B-ID_NUM')\n",
      "('8', 'B-ID_NUM')\n",
      "('1', 'B-ID_NUM')\n",
      "('6', 'B-ID_NUM')\n",
      "('1', 'B-ID_NUM')\n",
      "('8', 'B-ID_NUM')\n",
      "('4', 'B-ID_NUM')\n",
      "('9', 'B-ID_NUM')\n",
      "('5', 'B-ID_NUM')\n",
      "('9', 'B-ID_NUM')\n",
      "('3', 'B-ID_NUM')\n",
      "('1', 'B-ID_NUM')\n",
      "('▁', 'B-PHONE_NUM')\n",
      "('3', 'B-PHONE_NUM')\n",
      "('2', 'B-PHONE_NUM')\n",
      "('6', 'B-PHONE_NUM')\n",
      "('8', 'B-PHONE_NUM')\n",
      "('5', 'B-PHONE_NUM')\n",
      "('4', 'B-PHONE_NUM')\n",
      "('2', 'B-PHONE_NUM')\n",
      "('3', 'B-PHONE_NUM')\n",
      "('5', 'B-PHONE_NUM')\n",
      "('1', 'B-PHONE_NUM')\n",
      "('▁http', 'B-URL_PERSONAL')\n",
      "('://', 'B-URL_PERSONAL')\n",
      "('www', 'B-URL_PERSONAL')\n",
      "('.', 'B-URL_PERSONAL')\n",
      "('inst', 'B-URL_PERSONAL')\n",
      "('agram', 'B-URL_PERSONAL')\n",
      "('.', 'B-URL_PERSONAL')\n",
      "('com', 'B-URL_PERSONAL')\n",
      "('/', 'B-URL_PERSONAL')\n",
      "('z', 'B-URL_PERSONAL')\n",
      "('law', 'B-URL_PERSONAL')\n",
      "('rence', 'B-URL_PERSONAL')\n",
      "('▁', 'B-STREET_ADDRESS')\n",
      "('6', 'B-STREET_ADDRESS')\n",
      "('0', 'B-STREET_ADDRESS')\n",
      "('0', 'B-STREET_ADDRESS')\n",
      "('▁Jeff', 'I-STREET_ADDRESS')\n",
      "('ery', 'I-STREET_ADDRESS')\n",
      "('▁Park', 'I-STREET_ADDRESS')\n",
      "('ways', 'I-STREET_ADDRESS')\n",
      "('▁New', 'I-STREET_ADDRESS')\n",
      "('▁Jam', 'I-STREET_ADDRESS')\n",
      "('ess', 'I-STREET_ADDRESS')\n",
      "('ide', 'I-STREET_ADDRESS')\n",
      "(',', 'I-STREET_ADDRESS')\n",
      "('▁MT', 'I-STREET_ADDRESS')\n",
      "('▁', 'I-STREET_ADDRESS')\n",
      "('2', 'I-STREET_ADDRESS')\n",
      "('9', 'I-STREET_ADDRESS')\n",
      "('3', 'I-STREET_ADDRESS')\n",
      "('9', 'I-STREET_ADDRESS')\n",
      "('4', 'I-STREET_ADDRESS')\n",
      "('▁Daniel', 'B-NAME_STUDENT')\n",
      "('le', 'B-NAME_STUDENT')\n",
      "('▁Johnson', 'I-NAME_STUDENT')\n",
      "('▁Daniel', 'B-NAME_STUDENT')\n",
      "('le', 'B-NAME_STUDENT')\n",
      "('▁Johnson', 'I-NAME_STUDENT')\n",
      "('▁Daniel', 'B-NAME_STUDENT')\n",
      "('le', 'B-NAME_STUDENT')\n",
      "('▁Johnson', 'I-NAME_STUDENT')\n",
      "('▁Daniel', 'B-NAME_STUDENT')\n",
      "('le', 'B-NAME_STUDENT')\n",
      "('▁Johnson', 'I-NAME_STUDENT')\n",
      "('▁Daniel', 'B-NAME_STUDENT')\n",
      "('le', 'B-NAME_STUDENT')\n",
      "('▁Johnson', 'I-NAME_STUDENT')\n"
     ]
    }
   ],
   "source": [
    "x = train_ds[0]\n",
    "\n",
    "for t,l in zip(x[\"tokens\"], x[\"provided_labels\"]):\n",
    "    if l != \"O\":\n",
    "        print((t,l))\n",
    "\n",
    "print(\"*\"*100)\n",
    "\n",
    "for t, l in zip(tokenizer.convert_ids_to_tokens(x[\"input_ids\"]), x[\"labels\"]):\n",
    "    if id2label[l] != \"O\":\n",
    "        print((t,id2label[l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import recall_score, precision_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "import evaluate\n",
    "seqeval_metrics = evaluate.load(\"seqeval\")\n",
    "\n",
    "def f5_score(precision, recall):\n",
    "    return (1 + 5*5) * recall * precision / (5*5*precision + recall + 1e-100)\n",
    "\n",
    "def compute_metrics_from_labels(predictions, labels, id2label=id2label):\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval_metrics.compute(predictions=true_predictions, references=true_labels)\n",
    "    for label, scores in results.items():\n",
    "        if \"overall\" not in label:\n",
    "            precision = scores[\"precision\"]\n",
    "            recall = scores[\"recall\"]\n",
    "            results[label][\"f5_score\"] = f5_score(precision, recall)\n",
    "    precision = results[\"overall_precision\"]\n",
    "    recall = results[\"overall_recall\"]\n",
    "    results[\"overall_f5_score\"] = f5_score(precision, recall)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return compute_metrics_from_labels(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import MistralModel, MistralPreTrainedModel, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from typing import Optional, Union, List, Tuple\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from peft import prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BNB_CONFIG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "PEFT_CONFIG = LoraConfig(task_type=None, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.0, bias=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralForTokenClassification(MistralPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        config.quantization_config = BNB_CONFIG\n",
    "        \n",
    "        mistral = AutoModelForCausalLM.from_config(config)\n",
    "        mistral = prepare_model_for_kbit_training(mistral)\n",
    "\n",
    "        self.mistral = get_peft_model(mistral, PEFT_CONFIG)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.mistral(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fn(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.39.3'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers; transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install -U transformers==4.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistral = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=BNB_CONFIG, resume_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.42it/s]\n",
      "Some weights of MistralForTokenClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['model.classifier.bias', 'model.classifier.weight', 'model.mistral.base_model.model.lm_head.weight', 'model.mistral.base_model.model.model.embed_tokens.weight', 'model.mistral.base_model.model.model.layers.0.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.0.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.0.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.0.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.0.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.0.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.0.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.1.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.1.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.1.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.1.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.1.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.1.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.1.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.10.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.10.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.10.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.10.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.10.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.10.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.10.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.11.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.11.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.11.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.11.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.11.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.11.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.11.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.12.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.12.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.12.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.12.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.12.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.12.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.12.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.13.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.13.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.13.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.13.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.13.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.13.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.13.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.14.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.14.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.14.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.14.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.14.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.14.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.14.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.15.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.15.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.15.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.15.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.15.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.15.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.15.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.16.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.16.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.16.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.16.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.16.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.16.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.16.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.17.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.17.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.17.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.17.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.17.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.17.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.17.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.18.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.18.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.18.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.18.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.18.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.18.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.18.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.19.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.19.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.19.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.19.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.19.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.19.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.19.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.2.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.2.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.2.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.2.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.2.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.2.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.2.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.20.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.20.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.20.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.20.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.20.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.20.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.20.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.21.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.21.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.21.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.21.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.21.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.21.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.21.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.22.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.22.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.22.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.22.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.22.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.22.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.22.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.23.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.23.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.23.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.23.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.23.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.23.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.23.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.24.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.24.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.24.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.24.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.24.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.24.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.24.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.25.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.25.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.25.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.25.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.25.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.25.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.25.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.26.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.26.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.26.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.26.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.26.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.26.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.26.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.27.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.27.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.27.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.27.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.27.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.27.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.27.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.28.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.28.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.28.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.28.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.28.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.28.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.28.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.29.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.29.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.29.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.29.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.29.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.29.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.29.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.3.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.3.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.3.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.3.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.3.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.3.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.3.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.30.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.30.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.30.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.30.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.30.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.30.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.30.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.31.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.31.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.31.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.31.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.31.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.31.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.31.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.4.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.4.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.4.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.4.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.4.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.4.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.4.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.5.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.5.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.5.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.5.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.5.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.5.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.5.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.6.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.6.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.6.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.6.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.6.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.6.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.6.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.7.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.7.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.7.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.7.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.7.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.7.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.7.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.8.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.8.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.8.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.8.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.8.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.8.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.8.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.9.input_layernorm.weight', 'model.mistral.base_model.model.model.layers.9.mlp.down_proj.weight', 'model.mistral.base_model.model.model.layers.9.mlp.gate_proj.weight', 'model.mistral.base_model.model.model.layers.9.mlp.up_proj.weight', 'model.mistral.base_model.model.model.layers.9.post_attention_layernorm.weight', 'model.mistral.base_model.model.model.layers.9.self_attn.k_proj.weight', 'model.mistral.base_model.model.model.layers.9.self_attn.o_proj.weight', 'model.mistral.base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight', 'model.mistral.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.mistral.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'model.mistral.base_model.model.model.norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = MistralForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_run_name = f\"mistral-peft-{MAX_LENGTH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=f'training_logs/{wandb_run_name}', \n",
    "    run_name=wandb_run_name,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    report_to=\"wandb\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy='steps',\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=50,\n",
    "    do_eval=True,\n",
    "    save_total_limit=3,\n",
    "    lr_scheduler_type='cosine',\n",
    "    metric_for_best_model=\"overall_f5_score\",\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=True,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=args, \n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collator, \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mt-ionov\u001b[0m (\u001b[33mdeeppavlov_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/cephfs/home/ionov/workspace/kaggle/pii/baseline/wandb/run-20240411_225047-hcubpmyx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deeppavlov_team/PII%20Data%20Detection/runs/hcubpmyx' target=\"_blank\">mistral-peft-1024</a></strong> to <a href='https://wandb.ai/deeppavlov_team/PII%20Data%20Detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deeppavlov_team/PII%20Data%20Detection' target=\"_blank\">https://wandb.ai/deeppavlov_team/PII%20Data%20Detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deeppavlov_team/PII%20Data%20Detection/runs/hcubpmyx' target=\"_blank\">https://wandb.ai/deeppavlov_team/PII%20Data%20Detection/runs/hcubpmyx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libibverbs: Warning: couldn't open config directory '/etc/libibverbs.d'.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1561, in _call_impl\n    result = forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_3721170/250094790.py\", line 32, in forward\n    outputs = self.mistral(\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/peft/peft_model.py\", line 563, in forward\n    return self.get_base_model()(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1157, in forward\n    outputs = self.model(\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1042, in forward\n    layer_outputs = decoder_layer(\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 770, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 179, in forward\n    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 53.50 MiB is free. Process 1370598 has 4.05 GiB memory in use. Including non-PyTorch memory, this process has 35.26 GiB memory in use. Of the allocated memory 34.23 GiB is allocated by PyTorch, and 238.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/trainer.py:3036\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3036\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3039\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/trainer.py:3059\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3059\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:108\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    106\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 108\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1561, in _call_impl\n    result = forward_call(*args, **kwargs)\n  File \"/tmp/ipykernel_3721170/250094790.py\", line 32, in forward\n    outputs = self.mistral(\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/peft/peft_model.py\", line 563, in forward\n    return self.get_base_model()(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1157, in forward\n    outputs = self.model(\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 1042, in forward\n    layer_outputs = decoder_layer(\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 770, in forward\n    hidden_states = self.mlp(hidden_states)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ionov/anaconda3/envs/kaggle/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py\", line 179, in forward\n    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 39.39 GiB of which 53.50 MiB is free. Process 1370598 has 4.05 GiB memory in use. Including non-PyTorch memory, this process has 35.26 GiB memory in use. Of the allocated memory 34.23 GiB is allocated by PyTorch, and 238.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(f\"nuner_{MAX_LEG}\")\n",
    "# tokenizer.save_pretrained(\"mistal-peft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "sourceId": 163088908,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "kaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
